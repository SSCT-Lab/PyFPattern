

def clarkson_woodruff_transform(input_matrix, sketch_size, seed=None):
    '"\n    Applies a Clarkson-Woodruff Transform/sketch to the input matrix.\n\n    Given an input_matrix ``A`` of size ``(n, d)``, compute a matrix ``A\'`` of\n    size (sketch_size, d) so that\n\n    .. math:: \\|Ax\\| \\approx \\|A\'x\\|\n\n    with high probability via the Clarkson-Woodruff Transform, otherwise\n    known as the CountSketch matrix.\n\n    Parameters\n    ----------\n    input_matrix: array_like\n        Input matrix, of shape ``(n, d)``.\n    sketch_size: int\n        Number of rows for the sketch.\n    seed : None or int or `numpy.random.RandomState` instance, optional\n        This parameter defines the ``RandomState`` object to use for drawing\n        random variates.\n        If None (or ``np.random``), the global ``np.random`` state is used.\n        If integer, it is used to seed the local ``RandomState`` instance.\n        Default is None.\n\n    Returns\n    -------\n    A\' : array_like\n        Sketch of the input matrix ``A``, of size ``(sketch_size, d)``.\n\n    Notes\n    -----\n    To make the statement\n\n    .. math:: \\|Ax\\| \\approx \\|A\'x\\|\n\n    precise, observe the following result which is adapted from the\n    proof of Theorem 14 of [2]_ via Markov\'s Inequality. If we have\n    a sketch size ``sketch_size=k`` which is at least\n\n    .. math:: k \\geq \\frac{2}{\\epsilon^2\\delta}\n\n    Then for any fixed vector ``x``,\n\n    .. math:: \\|Ax\\| = (1\\pm\\epsilon)\\|A\'x\\|\n\n    with probability at least one minus delta.\n\n    This implementation takes advantage of sparsity: computing\n    a sketch takes time proportional to ``A.nnz``. Data ``A`` which\n    is in ``scipy.sparse.csc_matrix`` format gives the quickest\n    computation time for sparse input.\n\n    >>> from scipy import linalg\n    >>> from scipy import sparse\n    >>> n_rows, n_columns, density, sketch_n_rows = 15000, 100, 0.01, 200\n    >>> A = sparse.rand(n_rows, n_columns, density=density, format=\'csc\')\n    >>> B = sparse.rand(n_rows, n_columns, density=density, format=\'csr\')\n    >>> C = sparse.rand(n_rows, n_columns, density=density, format=\'coo\')\n    >>> D = np.random.randn(n_rows, n_columns)\n    >>> SA = linalg.clarkson_woodruff_transform(A, sketch_n_rows) # fastest\n    >>> SB = linalg.clarkson_woodruff_transform(B, sketch_n_rows) # fast\n    >>> SC = linalg.clarkson_woodruff_transform(C, sketch_n_rows) # slower\n    >>> SD = linalg.clarkson_woodruff_transform(D, sketch_n_rows) # slowest\n\n    That said, this method does perform well on dense inputs, just slower\n    on a relative scale.\n\n    Examples\n    --------\n    Given a big dense matrix ``A``:\n\n    >>> from scipy import linalg\n    >>> n_rows, n_columns, sketch_n_rows = 15000, 100, 200\n    >>> A = np.random.randn(n_rows, n_columns)\n    >>> sketch = linalg.clarkson_woodruff_transform(A, sketch_n_rows)\n    >>> sketch.shape\n    (200, 100)\n    >>> norm_A = np.linalg.norm(A)\n    >>> norm_sketch = np.linalg.norm(sketch)\n\n    Now with high probability, the true norm ``norm_A`` is close to\n    the sketched norm ``norm_sketch`` in absolute value.\n\n    Similarly, applying our sketch preserves the solution to a linear\n    regression of :math:`\\min \\|Ax - b\\|`.\n\n    >>> from scipy import linalg\n    >>> n_rows, n_columns, sketch_n_rows = 15000, 100, 200\n    >>> A = np.random.randn(n_rows, n_columns)\n    >>> b = np.random.randn(n_rows)\n    >>> x = np.linalg.lstsq(A, b, rcond=None)\n    >>> Ab = np.hstack((A, b.reshape(-1,1)))\n    >>> SAb = linalg.clarkson_woodruff_transform(Ab, sketch_n_rows)\n    >>> SA, Sb = SAb[:,:-1], SAb[:,-1]\n    >>> x_sketched = np.linalg.lstsq(SA, Sb, rcond=None)\n\n    As with the matrix norm example, ``np.linalg.norm(A @ x - b)``\n    is close to ``np.linalg.norm(A @ x_sketched - b)`` with high\n    probability.\n\n    References\n    ----------\n    .. [1] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and\n           regression in input sparsity time. In STOC, 2013.\n\n    .. [2] David P. Woodruff. Sketching as a tool for numerical linear algebra.\n           In Foundations and Trends in Theoretical Computer Science, 2014.\n\n    '
    S = cwt_matrix(sketch_size, input_matrix.shape[0], seed)
    return S.dot(input_matrix)
