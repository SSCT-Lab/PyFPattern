def to_parquet(self, fname, engine='auto', compression='snappy', index=None, partition_cols=None, **kwargs):
    "\n        Write a DataFrame to the binary parquet format.\n\n        .. versionadded:: 0.21.0\n\n        This function writes the dataframe as a `parquet file\n        <https://parquet.apache.org/>`_. You can choose different parquet\n        backends, and have the option of compression. See\n        :ref:`the user guide <io.parquet>` for more details.\n\n        Parameters\n        ----------\n        fname : str\n            File path or Root Directory path. Will be used as Root Directory\n            path while writing a partitioned dataset.\n\n            .. versionchanged:: 0.24.0\n\n        engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n            Parquet library to use. If 'auto', then the option\n            ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n            behavior is to try 'pyarrow', falling back to 'fastparquet' if\n            'pyarrow' is unavailable.\n        compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\n            Name of the compression to use. Use ``None`` for no compression.\n        index : bool, default None\n            If ``True``, include the dataframe's index(es) in the file output.\n            If ``False``, they will not be written to the file.\n            If ``None``, similar to ``True`` the dataframe's index(es)\n            will be saved. However, instead of being saved as values,\n            the RangeIndex will be stored as a range in the metadata so it\n            doesn't require much space and is faster. Other indexes will\n            be included as columns in the file output.\n\n            .. versionadded:: 0.24.0\n\n        partition_cols : list, optional, default None\n            Column names by which to partition the dataset.\n            Columns are partitioned in the order they are given.\n\n            .. versionadded:: 0.24.0\n\n        **kwargs\n            Additional arguments passed to the parquet library. See\n            :ref:`pandas io <io.parquet>` for more details.\n\n        See Also\n        --------\n        read_parquet : Read a parquet file.\n        DataFrame.to_csv : Write a csv file.\n        DataFrame.to_sql : Write to a sql table.\n        DataFrame.to_hdf : Write to hdf.\n\n        Notes\n        -----\n        This function requires either the `fastparquet\n        <https://pypi.org/project/fastparquet>`_ or `pyarrow\n        <https://arrow.apache.org/docs/python/>`_ library.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n        >>> df.to_parquet('df.parquet.gzip',\n        ...               compression='gzip')  # doctest: +SKIP\n        >>> pd.read_parquet('df.parquet.gzip')  # doctest: +SKIP\n           col1  col2\n        0     1     3\n        1     2     4\n        "
    from pandas.io.parquet import to_parquet
    to_parquet(self, fname, engine, compression=compression, index=index, partition_cols=partition_cols, **kwargs)