def cross_entropy(dist1, dist2):
    'Computes Cross entropy.\n\n    For two continuous distributions :math:`p(x), q(x)`, it is expressed as\n\n    .. math::\n        H(p,q) = - \\int p(x) \\log q(x) dx\n\n    For two discrete distributions :math:`p(x), q(x)`, it is expressed as\n\n    .. math::\n        H(p,q) = - \\sum_x p(x) \\log q(x)\n\n    This function call :func:`~chainer.kl_divergence` and\n    :meth:`~chainer.Distribution.entropy` of ``dist1``. Therefore, it is\n    necessary to register KL divergence function with\n    :func:`~chainer.register_kl` decoartor and define\n    :meth:`~chainer.Distribution.entropy` in ``dist1``.\n\n    Args:\n        dist1(:class:`~chainer.Distribution`): Distribution to calculate cross\n            entropy :math:`p`. This is the first (left) operand of the cross\n            entropy.\n        dist2(:class:`~chainer.Distribution`): Distribution to calculate cross\n            entropy :math:`q`. This is the second (right) operand of the cross\n            entropy.\n\n    Returns:\n        ~chainer.Variable: Output variable representing cross entropy\n        :math:`H(p,q)`.\n\n    '
    return (dist1.entropy() + kl_divergence(dist1, dist2))