def fit(self, x, y, batch_size=32, epochs=10, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, **kwargs):
    'Trains the model for a fixed number of epochs.\n\n        # Arguments\n            x: input data, as a Numpy array or list of Numpy arrays\n                (if the model has multiple inputs).\n            y: labels, as a Numpy array.\n            batch_size: integer. Number of samples per gradient update.\n            epochs: integer. Number of epochs to train the model.\n                Note that in conjunction with initial_epoch, the parameter\n                epochs is to be understood as "final epoch". The model is\n                not trained for a number of steps given by epochs, but\n                until the epoch epochs is reached.\n            verbose: 0 for no logging to stdout,\n                1 for progress bar logging, 2 for one log line per epoch.\n            callbacks: list of `keras.callbacks.Callback` instances.\n                List of callbacks to apply during training.\n                See [callbacks](/callbacks).\n            validation_split: float (0. < x < 1).\n                Fraction of the data to use as held-out validation data.\n            validation_data: tuple (x_val, y_val) or tuple\n                (x_val, y_val, val_sample_weights) to be used as held-out\n                validation data. Will override validation_split.\n            shuffle: boolean or str (for \'batch\').\n                Whether to shuffle the samples at each epoch.\n                \'batch\' is a special option for dealing with the\n                limitations of HDF5 data; it shuffles in batch-sized chunks.\n            class_weight: dictionary mapping classes to a weight value,\n                used for scaling the loss function (during training only).\n            sample_weight: Numpy array of weights for\n                the training samples, used for scaling the loss function\n                (during training only). You can either pass a flat (1D)\n                Numpy array with the same length as the input samples\n                (1:1 mapping between weights and samples),\n                or in the case of temporal data,\n                you can pass a 2D array with shape (samples, sequence_length),\n                to apply a different weight to every timestep of every sample.\n                In this case you should make sure to specify\n                sample_weight_mode="temporal" in compile().\n            initial_epoch: Epoch at which to start training\n                (useful for resuming a previous training run).\n\n        # Returns\n            A `History` object. Its `History.history` attribute is\n            a record of training loss values and metrics values\n            at successive epochs, as well as validation loss values\n            and validation metrics values (if applicable).\n\n        # Raises\n            RuntimeError: if the model was never compiled.\n        '
    if ('nb_epoch' in kwargs):
        warnings.warn('The `nb_epoch` argument in `fit` has been renamed `epochs`.')
        epochs = kwargs.pop('nb_epoch')
    if kwargs:
        raise TypeError(('Unrecognized keyword arguments: ' + str(kwargs)))
    if (not self.built):
        raise RuntimeError('The model needs to be compiled before being used.')
    return self.model.fit(x, y, batch_size=batch_size, epochs=epochs, verbose=verbose, callbacks=callbacks, validation_split=validation_split, validation_data=validation_data, shuffle=shuffle, class_weight=class_weight, sample_weight=sample_weight, initial_epoch=initial_epoch)