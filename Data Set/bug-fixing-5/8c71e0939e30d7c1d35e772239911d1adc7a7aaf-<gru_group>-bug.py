@wrap_name_default('gru_group')
def gru_group(input, memory_boot=None, size=None, name=None, reverse=False, gru_bias_attr=None, gru_param_attr=None, act=None, gate_act=None, gru_layer_attr=None, naive=False):
    '\n    gru_group is a recurrent_group version of Gated Recurrent Unit. It\n    does exactly the same calculation as the grumemory layer does. A promising\n    benefit is that gru hidden states are accessible to the user. This is\n    especially useful in attention model. If you do not need to access\n    any internal state, but merely use the outputs of a GRU, it is recommended\n    to use the grumemory, which is relatively faster.\n\n    Please see grumemory in layers.py for more detail about the maths.\n\n    The example usage is:\n\n    ..  code-block:: python\n\n        gru = gur_group(input=[layer1],\n                        size=256,\n                        act=TanhActivation(),\n                        gate_act=SigmoidActivation())\n\n    :param input: input layer name.\n    :type input: LayerOutput\n    :param memory_boot: the initialization state of the LSTM cell.\n    :type memory_boot: LayerOutput | None\n    :param name: name of the gru group.\n    :type name: basestring\n    :param size: hidden size of the gru.\n    :type size: int\n    :param reverse: whether to process the input data in a reverse order\n    :type reverse: bool\n    :param act: type of the activiation\n    :type act: BaseActivation\n    :param gate_act: type of the gate activiation\n    :type gate_act: BaseActivation\n    :param gru_bias_attr: bias. False means no bias, None means default bias.\n    :type gru_bias_attr: ParameterAttribute|False\n    :param gru_layer_attr: Extra parameter attribute of the gru layer.\n    :type gru_layer_attr: ParameterAttribute|False\n    :return: the gru group.\n    :rtype: LayerOutput\n    '

    def __gru_step__(ipt):
        return gru_unit(input=ipt, memory_boot=memory_boot, name=name, size=size, gru_bias_attr=gru_bias_attr, gru_param_attr=gru_param_attr, act=act, gate_act=gate_act, gru_layer_attr=gru_layer_attr, naive=naive)
    return recurrent_group(name=('%s_recurrent_group' % name), step=__gru_step__, reverse=reverse, input=input)