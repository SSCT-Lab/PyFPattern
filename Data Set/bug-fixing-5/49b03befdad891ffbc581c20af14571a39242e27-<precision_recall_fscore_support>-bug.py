def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn'):
    'Compute precision, recall, F-measure and support for each class\n\n    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n    true positives and ``fp`` the number of false positives. The precision is\n    intuitively the ability of the classifier not to label as positive a sample\n    that is negative.\n\n    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n    true positives and ``fn`` the number of false negatives. The recall is\n    intuitively the ability of the classifier to find all the positive samples.\n\n    The F-beta score can be interpreted as a weighted harmonic mean of\n    the precision and recall, where an F-beta score reaches its best\n    value at 1 and worst score at 0.\n\n    The F-beta score weights recall more than precision by a factor of\n    ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n\n    The support is the number of occurrences of each class in ``y_true``.\n\n    If ``pos_label is None`` and in binary classification, this function\n    returns the average precision, recall and F-measure if ``average``\n    is one of ``\'micro\'``, ``\'macro\'``, ``\'weighted\'`` or ``\'samples\'``.\n\n    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n\n    Parameters\n    ----------\n    y_true : 1d array-like, or label indicator array / sparse matrix\n        Ground truth (correct) target values.\n\n    y_pred : 1d array-like, or label indicator array / sparse matrix\n        Estimated targets as returned by a classifier.\n\n    beta : float, 1.0 by default\n        The strength of recall versus precision in the F-score.\n\n    labels : list, optional\n        The set of labels to include when ``average != \'binary\'``, and their\n        order if ``average is None``. Labels present in the data can be\n        excluded, for example to calculate a multiclass average ignoring a\n        majority negative class, while labels not present in the data will\n        result in 0 components in a macro average. For multilabel targets,\n        labels are column indices. By default, all labels in ``y_true`` and\n        ``y_pred`` are used in sorted order.\n\n    pos_label : str or int, 1 by default\n        The class to report if ``average=\'binary\'`` and the data is binary.\n        If the data are multiclass or multilabel, this will be ignored;\n        setting ``labels=[pos_label]`` and ``average != \'binary\'`` will report\n        scores for that label only.\n\n    average : string, [None (default), \'binary\', \'micro\', \'macro\', \'samples\',                        \'weighted\']\n        If ``None``, the scores for each class are returned. Otherwise, this\n        determines the type of averaging performed on the data:\n\n        ``\'binary\'``:\n            Only report results for the class specified by ``pos_label``.\n            This is applicable only if targets (``y_{true,pred}``) are binary.\n        ``\'micro\'``:\n            Calculate metrics globally by counting the total true positives,\n            false negatives and false positives.\n        ``\'macro\'``:\n            Calculate metrics for each label, and find their unweighted\n            mean.  This does not take label imbalance into account.\n        ``\'weighted\'``:\n            Calculate metrics for each label, and find their average weighted\n            by support (the number of true instances for each label). This\n            alters \'macro\' to account for label imbalance; it can result in an\n            F-score that is not between precision and recall.\n        ``\'samples\'``:\n            Calculate metrics for each instance, and find their average (only\n            meaningful for multilabel classification where this differs from\n            :func:`accuracy_score`).\n\n    warn_for : tuple or set, for internal use\n        This determines which warnings will be made in the case that this\n        function is being used to return only one of its metrics.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    zero_division : "warn", 0 or 1, default="warn"\n        Sets the value to return when there is a zero division:\n           - recall: when there are no positive labels\n           - precision: when there are no positive predictions\n           - f-score: both\n\n        If set to "warn", this acts as 0, but warnings are also raised.\n\n    Returns\n    -------\n    precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n\n    recall : float (if average is not None) or array of float, , shape =        [n_unique_labels]\n\n    fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n\n    support : int (if average is not None) or array of int, shape =        [n_unique_labels]\n        The number of occurrences of each label in ``y_true``.\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for the Precision and recall\n           <https://en.wikipedia.org/wiki/Precision_and_recall>`_\n\n    .. [2] `Wikipedia entry for the F1-score\n           <https://en.wikipedia.org/wiki/F1_score>`_\n\n    .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n           in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n           Godbole, Sunita Sarawagi\n           <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics import precision_recall_fscore_support\n    >>> y_true = np.array([\'cat\', \'dog\', \'pig\', \'cat\', \'dog\', \'pig\'])\n    >>> y_pred = np.array([\'cat\', \'pig\', \'dog\', \'cat\', \'cat\', \'dog\'])\n    >>> precision_recall_fscore_support(y_true, y_pred, average=\'macro\')\n    (0.22..., 0.33..., 0.26..., None)\n    >>> precision_recall_fscore_support(y_true, y_pred, average=\'micro\')\n    (0.33..., 0.33..., 0.33..., None)\n    >>> precision_recall_fscore_support(y_true, y_pred, average=\'weighted\')\n    (0.22..., 0.33..., 0.26..., None)\n\n    It is possible to compute per-label precisions, recalls, F1-scores and\n    supports instead of averaging:\n\n    >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n    ... labels=[\'pig\', \'dog\', \'cat\'])\n    (array([0.        , 0.        , 0.66...]),\n     array([0., 0., 1.]), array([0. , 0. , 0.8]),\n     array([2, 2, 2]))\n\n    Notes\n    -----\n    When ``true positive + false positive == 0``, precision is undefined;\n    When ``true positive + false negative == 0``, recall is undefined.\n    In such cases, by default the metric will be set to 0, as will f-score,\n    and ``UndefinedMetricWarning`` will be raised. This behavior can be\n    modified with ``zero_division``.\n    '
    _check_zero_division(zero_division)
    if (beta < 0):
        raise ValueError('beta should be >=0 in the F-beta score')
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
    samplewise = (average == 'samples')
    MCM = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight, labels=labels, samplewise=samplewise)
    tp_sum = MCM[:, 1, 1]
    pred_sum = (tp_sum + MCM[:, 0, 1])
    true_sum = (tp_sum + MCM[:, 1, 0])
    if (average == 'micro'):
        tp_sum = np.array([tp_sum.sum()])
        pred_sum = np.array([pred_sum.sum()])
        true_sum = np.array([true_sum.sum()])
    beta2 = (beta ** 2)
    precision = _prf_divide(tp_sum, pred_sum, 'precision', 'predicted', average, warn_for, zero_division)
    recall = _prf_divide(tp_sum, true_sum, 'recall', 'true', average, warn_for, zero_division)
    if ((zero_division == 'warn') and (('f-score',) == warn_for)):
        if (pred_sum[(true_sum == 0)] == 0).any():
            _warn_prf(average, 'true nor predicted', 'F-score is', len(true_sum))
    if np.isposinf(beta):
        f_score = recall
    else:
        denom = ((beta2 * precision) + recall)
        denom[(denom == 0.0)] = 1
        f_score = ((((1 + beta2) * precision) * recall) / denom)
    if (average == 'weighted'):
        weights = true_sum
        if (weights.sum() == 0):
            zero_division_value = (0.0 if (zero_division in ['warn', 0]) else 1.0)
            return ((zero_division_value if (pred_sum.sum() == 0) else 0), zero_division_value, (zero_division_value if (pred_sum.sum() == 0) else 0), None)
    elif (average == 'samples'):
        weights = sample_weight
    else:
        weights = None
    if (average is not None):
        assert ((average != 'binary') or (len(precision) == 1))
        precision = np.average(precision, weights=weights)
        recall = np.average(recall, weights=weights)
        f_score = np.average(f_score, weights=weights)
        true_sum = None
    return (precision, recall, f_score, true_sum)