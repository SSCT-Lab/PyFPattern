@tf_export(v1=['gradients'])
def gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None, stop_gradients=None, unconnected_gradients=UnconnectedGradients.NONE):
    "Constructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`.\n\n  `ys` and `xs` are each a `Tensor` or a list of tensors.  `grad_ys`\n  is a list of `Tensor`, holding the gradients received by the\n  `ys`. The list must be the same length as `ys`.\n\n  `gradients()` adds ops to the graph to output the derivatives of `ys` with\n  respect to `xs`.  It returns a list of `Tensor` of length `len(xs)` where\n  each tensor is the `sum(dy/dx)` for y in `ys`.\n\n  `grad_ys` is a list of tensors of the same length as `ys` that holds\n  the initial gradients for each y in `ys`.  When `grad_ys` is None,\n  we fill in a tensor of '1's of the shape of y for each y in `ys`.  A\n  user can provide their own initial `grad_ys` to compute the\n  derivatives using a different initial gradient for each y (e.g., if\n  one wanted to weight the gradient differently for each value in\n  each y).\n\n  `stop_gradients` is a `Tensor` or a list of tensors to be considered constant\n  with respect to all `xs`. These tensors will not be backpropagated through,\n  as though they had been explicitly disconnected using `stop_gradient`.  Among\n  other things, this allows computation of partial derivatives as opposed to\n  total derivatives. For example:\n\n  ```python\n  a = tf.constant(0.)\n  b = 2 * a\n  g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])\n  ```\n\n  Here the partial derivatives `g` evaluate to `[1.0, 1.0]`, compared to the\n  total derivatives `tf.gradients(a + b, [a, b])`, which take into account the\n  influence of `a` on `b` and evaluate to `[3.0, 1.0]`.  Note that the above is\n  equivalent to:\n\n  ```python\n  a = tf.stop_gradient(tf.constant(0.))\n  b = tf.stop_gradient(2 * a)\n  g = tf.gradients(a + b, [a, b])\n  ```\n\n  `stop_gradients` provides a way of stopping gradient after the graph has\n  already been constructed, as compared to `tf.stop_gradient` which is used\n  during graph construction.  When the two approaches are combined,\n  backpropagation stops at both `tf.stop_gradient` nodes and nodes in\n  `stop_gradients`, whichever is encountered first.\n\n  All integer tensors are considered constant with respect to all `xs`, as if\n  they were included in `stop_gradients`.\n\n  `unconnected_gradients` determines the value returned for each x in xs if it\n  is unconnected in the graph to ys. By default this is None to safeguard\n  against errors. Mathematically these gradients are zero which can be requested\n  using the `'zero'` option. `tf.UnconnectedGradients` provides the\n  following options and behaviors:\n\n  ```python\n  a = tf.ones([1, 2])\n  b = tf.ones([3, 1])\n  g1 = tf.gradients([b], [a], unconnected_gradients='none')\n  sess.run(g1)  # [None]\n\n  g2 = tf.gradients([b], [a], unconnected_gradients='zero')\n  sess.run(g2)  # [array([[0., 0.]], dtype=float32)]\n  ```\n\n\n  Args:\n    ys: A `Tensor` or list of tensors to be differentiated.\n    xs: A `Tensor` or list of tensors to be used for differentiation.\n    grad_ys: Optional. A `Tensor` or list of tensors the same size as\n      `ys` and holding the gradients computed for each y in `ys`.\n    name: Optional name to use for grouping all the gradient ops together.\n      defaults to 'gradients'.\n    colocate_gradients_with_ops: If True, try colocating gradients with\n      the corresponding op.\n    gate_gradients: If True, add a tuple around the gradients returned\n      for an operations.  This avoids some race conditions.\n    aggregation_method: Specifies the method used to combine gradient terms.\n      Accepted values are constants defined in the class `AggregationMethod`.\n    stop_gradients: Optional. A `Tensor` or list of tensors not to differentiate\n      through.\n    unconnected_gradients: Optional. Specifies the gradient value returned when\n      the given input tensors are unconnected. Accepted values are constants\n      defined in the class `tf.UnconnectedGradients` and the default value is\n      `none`.\n\n  Returns:\n    A list of `sum(dy/dx)` for each x in `xs`.\n\n  Raises:\n    LookupError: if one of the operations between `x` and `y` does not\n      have a registered gradient function.\n    ValueError: if the arguments are invalid.\n    RuntimeError: if called in Eager mode.\n\n  "
    with ops.get_default_graph()._mutation_lock():
        return gradients_util._GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)